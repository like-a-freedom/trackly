groups:
  - name: trackly_backend_health
    interval: 30s
    rules:
      - alert: BackendDown
        expr: up{job="trackly_backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "Trackly backend is down"
          description: "Backend service has been down for more than 1 minute. Service: {{ $labels.instance }}"

      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_errors_total{job="trackly_backend"}[5m])) by (service, route) 
          / 
          sum(rate(http_requests_total{job="trackly_backend"}[5m])) by (service, route) 
          > 0.05
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} on route {{ $labels.route }}"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{job="trackly_backend"}[5m])) by (le, route)
          ) > 2
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High latency on {{ $labels.route }}"
          description: "95th percentile latency is {{ $value }}s on route {{ $labels.route }}"

      - alert: VeryHighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{job="trackly_backend"}[5m])) by (le, route)
          ) > 5
        for: 2m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "Very high latency on {{ $labels.route }}"
          description: "95th percentile latency is {{ $value }}s on route {{ $labels.route }}"

  - name: trackly_database_health
    interval: 30s
    rules:
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          db_pool_connections{state="in_use"} / db_pool_connections{state="max"} > 0.9
        for: 2m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "{{ $value | humanizePercentage }} of database connections are in use"

      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95, 
            sum(rate(db_query_duration_seconds_bucket[5m])) by (le, operation)
          ) > 1
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "95th percentile query time for {{ $labels.operation }} is {{ $value }}s"

  - name: trackly_track_processing
    interval: 30s
    rules:
      - alert: HighTrackUploadFailureRate
        expr: |
          sum(rate(track_upload_failures_total[5m])) by (reason) 
          / 
          (sum(rate(tracks_uploaded_total[5m])) + sum(rate(track_upload_failures_total[5m])))
          > 0.1
        for: 5m
        labels:
          severity: warning
          component: track_processing
        annotations:
          summary: "High track upload failure rate"
          description: "{{ $value | humanizePercentage }} of uploads failing due to {{ $labels.reason }}"

      - alert: SlowTrackParsing
        expr: |
          histogram_quantile(0.95, 
            sum(rate(track_parse_duration_seconds_bucket[5m])) by (le, format)
          ) > 5
        for: 5m
        labels:
          severity: warning
          component: track_processing
        annotations:
          summary: "Slow track parsing for {{ $labels.format }}"
          description: "95th percentile parsing time is {{ $value }}s"

      - alert: ElevationEnrichmentFailures
        expr: |
          sum(rate(track_enrich_requests_total{status="failed_remote"}[10m]))
          /
          sum(rate(track_enrich_requests_total[10m]))
          > 0.3
        for: 5m
        labels:
          severity: warning
          component: elevation
        annotations:
          summary: "High elevation enrichment failure rate"
          description: "{{ $value | humanizePercentage }} of enrichment requests are failing"

      - alert: SlowElevationEnrichment
        expr: |
          histogram_quantile(0.95, 
            sum(rate(track_enrich_duration_seconds_bucket{status="success"}[5m])) by (le)
          ) > 10
        for: 5m
        labels:
          severity: warning
          component: elevation
        annotations:
          summary: "Slow elevation enrichment"
          description: "95th percentile enrichment time is {{ $value }}s"
      - alert: EnrichmentQueueFull
        expr: |
          increase(track_enrich_requests_total{job="trackly_backend", status="queue_full"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          component: elevation
        annotations:
          summary: "Enrichment queue full events"
          description: "Enrichment queue fallback occurred {{ $value }} times in the last 5m"

      - alert: HighElevationAPICalls
        expr: |
          sum(increase(elevation_api_calls_total{job="trackly_backend"}[5m])) > 500
        for: 5m
        labels:
          severity: warning
          component: elevation
        annotations:
          summary: "High elevation API call rate"
          description: "Elevation calls increased by {{ $value }} over 5 minutes"

      - alert: SlowPOILink
        expr: |
          histogram_quantile(0.95, sum(rate(track_poi_link_duration_seconds_bucket{job="trackly_backend"}[5m])) by (le, operation)) > 1
        for: 5m
        labels:
          severity: warning
          component: poi
        annotations:
          summary: "Slow POI linking"
          description: "95th percentile POI linking time is {{ $value }}s for operation {{ $labels.operation }}"

  - name: trackly_resource_usage
    interval: 30s
    rules:
      - alert: HighMemoryUsage
        expr: |
          process_resident_memory_bytes{job="trackly_backend"} > 1073741824
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "High memory usage"
          description: "Backend is using {{ $value | humanize }}B of memory"

      - alert: TooManyBackgroundTasks
        expr: |
          background_tasks_in_flight > 50
        for: 5m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "Too many background tasks running"
          description: "{{ $value }} background tasks are currently running"

  - name: trackly_product_metrics
    interval: 1m
    rules:
      - alert: NoTracksUploadedRecently
        expr: |
          rate(tracks_uploaded_total[1h]) == 0
        for: 2h
        labels:
          severity: info
          component: product
        annotations:
          summary: "No tracks uploaded in the last 2 hours"
          description: "This might indicate a problem with the upload functionality or lack of user activity"

      - alert: HighTrackDeduplicationRate
        expr: |
          sum(rate(tracks_deduplicated_total[30m]))
          /
          (sum(rate(tracks_uploaded_total[30m])) + sum(rate(tracks_deduplicated_total[30m])))
          > 0.5
        for: 30m
        labels:
          severity: info
          component: product
        annotations:
          summary: "High track deduplication rate"
          description: "{{ $value | humanizePercentage }} of track uploads are duplicates"

  - name: trackly_frontend_health
    interval: 30s
    rules:
      - alert: FrontendDown
        expr: up{job="trackly_frontend"} == 0
        for: 1m
        labels:
          severity: critical
          component: frontend
        annotations:
          summary: "Trackly frontend (Caddy) is down"
          description: "Frontend reverse proxy has been down for more than 1 minute. Service: {{ $labels.instance }}"

      - alert: FrontendHighErrorRate
        expr: |
          sum(rate(caddy_http_requests_total{job="trackly_frontend", code=~"5.."}[5m]))
          /
          sum(rate(caddy_http_requests_total{job="trackly_frontend"}[5m]))
          > 0.05
        for: 5m
        labels:
          severity: warning
          component: frontend
        annotations:
          summary: "High error rate on frontend"
          description: "Frontend error rate is {{ $value | humanizePercentage }}"

      - alert: FrontendHighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(caddy_http_request_duration_seconds_bucket{job="trackly_frontend"}[5m])) by (le)
          ) > 3
        for: 5m
        labels:
          severity: warning
          component: frontend
        annotations:
          summary: "High frontend latency"
          description: "95th percentile frontend latency is {{ $value }}s"

      - alert: BackendUpstreamUnhealthy
        expr: |
          caddy_reverse_proxy_upstreams_healthy{job="trackly_frontend"} == 0
        for: 1m
        labels:
          severity: critical
          component: frontend
        annotations:
          summary: "Backend upstream is unhealthy"
          description: "Caddy reports backend upstream as unhealthy for {{ $labels.upstream }}"

      - alert: FrontendHighMemoryUsage
        expr: |
          process_resident_memory_bytes{job="trackly_frontend"} > 536870912
        for: 5m
        labels:
          severity: warning
          component: frontend
        annotations:
          summary: "High memory usage on frontend"
          description: "Frontend (Caddy) is using {{ $value | humanize }}B of memory"

      - alert: FrontendHighGoroutines
        expr: |
          go_goroutines{job="trackly_frontend"} > 1000
        for: 5m
        labels:
          severity: warning
          component: frontend
        annotations:
          summary: "High number of goroutines on frontend"
          description: "Frontend has {{ $value }} goroutines running"
